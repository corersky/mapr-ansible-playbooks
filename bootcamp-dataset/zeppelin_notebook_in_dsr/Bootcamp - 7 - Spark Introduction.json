{"paragraphs":[{"text":"%md\n## Spark notebook\nThis Spark notebook is written in Scala which is a JVM language. It is also possible to write Spark notebooks in Python and R but this is not covered in this Lab. The first time a paragraph is executed it will request resources from the cluster (using YARN) which takes some time. Subsequent paragraphs will be faster as resources have been registered.","dateUpdated":"2018-07-03T12:24:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739550_-756636894","id":"20180223-132950_1191843637","dateCreated":"2018-07-03T12:08:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1262","user":"mapr","dateFinished":"2018-07-03T12:24:38+0000","dateStarted":"2018-07-03T12:24:38+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Spark notebook</h2>\n<p>This Spark notebook is written in Scala which is a JVM language. It is also possible to write Spark notebooks in Python and R but this is not covered in this Lab. The first time a paragraph is executed it will request resources from the cluster (using YARN) which takes some time. Subsequent paragraphs will be faster as resources have been registered.</p>\n</div>"}]}},{"text":"%spark\n// This first paragraph simply prints the Spark version. This is usefull as it tells you which version of Spark you are using. It is possible to run different versions alongside each other within the same cluster\nprintln( spark.version )","user":"mapr","dateUpdated":"2018-07-03T12:19:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739551_-757021643","id":"20180221-173517_33190424","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:19:32+0000","dateFinished":"2018-07-03T12:19:33+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1263"},{"text":"%md\n### Data loading\nSome data need to be loaded to get started. Spark supports many data sources and formats like files (JSON, XML, CSV, etc.) and databases (relational and NoSQL). Data is read into a Dataframe object which is a data structure what represents a stronly typed table. Hence Dataframes have a schema (with optionally nested fields) and supports typical SQL operations like select, where and filter.\n","dateUpdated":"2018-07-03T12:24:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739551_-757021643","id":"20180305-115339_1483469096","dateCreated":"2018-07-03T12:08:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1264","user":"mapr","dateFinished":"2018-07-03T12:24:40+0000","dateStarted":"2018-07-03T12:24:40+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Data loading</h3>\n<p>Some data need to be loaded to get started. Spark supports many data sources and formats like files (JSON, XML, CSV, etc.) and databases (relational and NoSQL). Data is read into a Dataframe object which is a data structure what represents a stronly typed table. Hence Dataframes have a schema (with optionally nested fields) and supports typical SQL operations like select, where and filter.</p>\n</div>"}]}},{"text":"%spark\n// The next paragraph loads the JSON data from MapR-DB JSON as  created by the image classification service. Each record contains information on objects that have been recognized within an uploaded image. The JSON is parsed by Spark and put in a dataframe object. You get some insight into the data by printing the schema, record count and some example records \nimport com.mapr.db.spark._\n\nval image_tags = spark.loadFromMapRDB(\"/remote-image-classification/remote-image-output-table\")\nprintln(\"Total number of records: \"+image_tags.count())\nprintln(\"--- [ SCHEMA ] ---\")\nimage_tags.printSchema()","user":"mapr","dateUpdated":"2018-07-03T12:25:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739551_-757021643","id":"20180221-173522_812661784","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:25:31+0000","dateFinished":"2018-07-03T12:25:32+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1265"},{"text":"%spark\n// print the top 10 recods using the dataframe (creates ascii art table that is not always readable...) \nprintln(\"--- [ EXAMPLE TABLE (10 records) ] ---\")\nimage_tags.show(10)\n\n// print 1 recod using the RDD (the datastructure that sits under the hood of the databrame) which contains all information. This will show that a single image can contain many recognized objects.\nprintln(\"--- [ ONLY FIRST RECORD ] ---\")\nimage_tags.rdd.take(1).foreach(println)","user":"mapr","dateUpdated":"2018-07-03T12:20:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739552_-771257352","id":"20180227-170905_1789191727","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:20:38+0000","dateFinished":"2018-07-03T12:20:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1266"},{"text":"\n// Check the number of partitions of the data which reflects the paralelism used by Spark. Only one partition means there is no parallelisation at all and many partitions can be an indication of overhead\nprintln(image_tags.rdd.getNumPartitions)\n\n// the large number of partitions is the result of the large number of very small files. A smaller number of partitions is more suitable for this data so we change the partitioning\nval image_tags_repartitioned = image_tags.repartition(4).cache()\nprintln(image_tags_repartitioned.rdd.getNumPartitions)","user":"mapr","dateUpdated":"2018-07-03T12:22:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739552_-771257352","id":"20180223-143257_1663498152","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:22:40+0000","dateFinished":"2018-07-03T12:22:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1267"},{"text":"%md\n### Transformations\nAfter loading and checking the data it is common to transform it into something usable. This typically means selecting specific fields, removing rows (filter), transforming fields (using UDF's) or aggregating the data. The following paragraphs show some of these functions.","user":"mapr","dateUpdated":"2018-07-03T12:24:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739552_-771257352","id":"20180305-120151_1322289526","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:24:45+0000","dateFinished":"2018-07-03T12:24:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1268","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Transformations</h3>\n<p>After loading and checking the data it is common to transform it into something usable. This typically means selecting specific fields, removing rows (filter), transforming fields (using UDF&rsquo;s) or aggregating the data. The following paragraphs show some of these functions.</p>\n</div>"}]}},{"text":"%spark\n// The nested 'object' array is a bit inconvenient so lets 'explode' the objects array such that each recognized object will have its own record. This will increase the number of records\nimport org.apache.spark.sql.functions.explode\n\nval image_tags_expl = image_tags.withColumn(\"object\", explode($\"objects\")).drop(\"objects\")\nprintln(\"Total number of records: \"+ image_tags_expl.count )\nimage_tags_expl.printSchema()","user":"mapr","dateUpdated":"2018-07-03T12:23:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739552_-771257352","id":"20180221-174427_701014670","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:23:43+0000","dateFinished":"2018-07-03T12:23:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1269"},{"text":"%spark\n// Create a User Defined Function (UDF) in order to calculate the surface (pixels) of each recognized object using the bounding box information of each object\ndef surface(topleftX: Int, topleftY: Int, bottomrightX: Int, bottomrightY: Int): Float = {\n    (bottomrightX - topleftX) * (bottomrightY - topleftY)\n}\nval surfaceUDF = udf[Float, Int, Int, Int,Int](surface)\n\n// execute the UDF on the data and create a new dataframe containing the result\nval image_tags_surface = image_tags_expl.withColumn(\"surface\", surfaceUDF($\"object.topleft.x\",$\"object.topleft.y\",$\"object.bottomright.x\",$\"object.bottomright.y\" ))\n\n// print a selection of the fields to validate the results\nimage_tags_surface.select($\"object.topleft.x\",$\"object.topleft.y\",$\"object.bottomright.x\",$\"object.bottomright.y\", $\"surface\").show()","user":"mapr","dateUpdated":"2018-07-03T12:23:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739553_-771642101","id":"20180223-142233_395256744","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:23:49+0000","dateFinished":"2018-07-03T12:23:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1270"},{"text":"\n// aggregate the data programatically to get information per label\nimage_tags_surface.groupBy(\"object.label\").agg(avg(\"object.confidence\").alias(\"avg_confidence\"), count(lit(1)).alias(\"cnt\"), avg(\"surface\").alias(\"avg_surface\")).orderBy(desc(\"avg_confidence\")).show(5)\n\n// register the dataframe as a temporary view within Spark so it can be manipulated using SQL \nimage_tags_surface.createOrReplaceTempView(\"image_tags\")\n\n// create the exact same aggregation using SQL on the temporary view\nval sqlResult = spark.sql(\"\"\"SELECT object.label, avg(object.confidence) AS avg_confidence, count(1) as cnt, avg(surface) AS avg_surface\n    FROM image_tags\n    GROUP BY object.label\n    ORDER BY avg_confidence DESC\"\"\")\nsqlResult.show(5)\n","user":"mapr","dateUpdated":"2018-07-03T12:24:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739553_-771642101","id":"20180223-165349_411544882","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:24:02+0000","dateFinished":"2018-07-03T12:24:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1271"},{"text":"%md\n### SQL Interpreter\nOnce a table has been registered in memory it is possible to query it using the %sql interpreter. This interpreter has some visualisation options like bar, pie and line charts. ","dateUpdated":"2018-07-03T12:24:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739553_-771642101","id":"20180305-123341_1156956491","dateCreated":"2018-07-03T12:08:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1272","user":"mapr","dateFinished":"2018-07-03T12:24:49+0000","dateStarted":"2018-07-03T12:24:49+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>SQL Interpreter</h3>\n<p>Once a table has been registered in memory it is possible to query it using the %sql interpreter. This interpreter has some visualisation options like bar, pie and line charts.</p>\n</div>"}]}},{"text":"%sql\n-- This is the same aggregation as executed before. Feel free to replace this with other queries you can think of and execute these\nSELECT object.label, avg(object.confidence) AS avg_confidence, count(1) as cnt, avg(surface) AS avg_surface\n    FROM image_tags\n    GROUP BY object.label\n    ORDER BY avg_confidence DESC\n    LIMIT 100","user":"mapr","dateUpdated":"2018-07-03T12:24:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"scatterChart":{"yAxis":{"name":"avg_conf","index":1,"aggr":"sum"},"size":{"name":"cnt","index":2,"aggr":"sum"},"xAxis":{"name":"label","index":0,"aggr":"sum"}},"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"label","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"cnt","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739554_-770487854","id":"20180221-180717_389327571","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:24:10+0000","dateFinished":"2018-07-03T12:24:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1273"},{"text":"%md\n### Spark ML\nSpark has an extensive machine learning library which contains statistical methods as wel as ML algorithms. The following paragraphs use the Statistics library to calculate any correlation between object surface and image classification confidence.","dateUpdated":"2018-07-03T12:24:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739554_-770487854","id":"20180306-075524_1178577132","dateCreated":"2018-07-03T12:08:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1274","user":"mapr","dateFinished":"2018-07-03T12:24:52+0000","dateStarted":"2018-07-03T12:24:52+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark ML</h3>\n<p>Spark has an extensive machine learning library which contains statistical methods as wel as ML algorithms. The following paragraphs use the Statistics library to calculate any correlation between object surface and image classification confidence.</p>\n</div>"}]}},{"text":"%spark\n// calculate the correlation between surface and confidence for all the data\nimport org.apache.spark.mllib.stat.Statistics\n\n// create the input data needed by the correlation function which is a list of confidence numbers and list of surface numbers\nval confidence_rdd = image_tags_surface.select(\"object.confidence\").rdd.map(_.getDouble(0))\nval surface_rdd = image_tags_surface.select(\"surface\").rdd.map(_.getFloat(0)).map(_.toDouble)\n// optionally print the top 5 of each list to check the numbers\n// confidence_rdd.take(5).foreach(println)\n// surface_rdd.take(5).foreach(println)\n\n// calculate the correlation between surface and confidence, does the result make sense to you? Feel free to use 'spearman' as well (takes a bit longer)\nval correlation = Statistics.corr(surface_rdd, confidence_rdd, \"pearson\") // spearman is also supported\n","user":"mapr","dateUpdated":"2018-07-03T12:24:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739554_-770487854","id":"20180221-181556_203999444","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:24:19+0000","dateFinished":"2018-07-03T12:24:21+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1275"},{"text":"%spark\n\n// This paragraph gets the correlation per label to see if that has an effect. First get all labels with a count bigger than 100. Then prepare the surface and confidence lists for each label and calculate the correlation.\nval labels = spark.sql(\"SELECT object.label as label, count(1) as cnt FROM image_tags GROUP BY object.label HAVING cnt > 100 ORDER BY label ASC\").rdd.map(_.getString(0)).collect()\n\n// calculate the correlation between surface and confidence for each label\nfor (label <- labels){\n    val corr_data = image_tags_surface.filter(\"object.label = '\"+label+\"'\")\n    val confidence_rdd = corr_data.select(\"object.confidence\").rdd.map(_.getDouble(0))\n    val surface_rdd = corr_data.select(\"surface\").rdd.map(_.getFloat(0)).map(_.toDouble)\n    \n    println(\"Correlation for \"+label+\" = \" + Statistics.corr(confidence_rdd, surface_rdd, \"pearson\")+\" (based on \"+corr_data.count()+\" records\")\n}\n\n// Try altering the minimum count for objects and try again. Why does Spark throw an error when using a minimum count of 0?","user":"mapr","dateUpdated":"2018-07-03T12:24:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739554_-770487854","id":"20180227-194656_715643417","dateCreated":"2018-07-03T12:08:59+0000","dateStarted":"2018-07-03T12:24:25+0000","dateFinished":"2018-07-03T12:24:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1276"},{"text":"%md\n### That is it folks!\nFeel free to copy paragraphs from above and experiment with them. For example execute other SQL queries or try to find the correlation between the location of the object and recognition confidcence. I am sure you can think of other cools things to do :)\n","dateUpdated":"2018-07-03T12:24:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739555_-770872603","id":"20180227-202456_2090718954","dateCreated":"2018-07-03T12:08:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1277","user":"mapr","dateFinished":"2018-07-03T12:24:55+0000","dateStarted":"2018-07-03T12:24:55+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>That is it folks!</h3>\n<p>Feel free to copy paragraphs from above and experiment with them. For example execute other SQL queries or try to find the correlation between the location of the object and recognition confidcence. I am sure you can think of other cools things to do :)</p>\n</div>"}]}},{"text":"%spark\n// just put your spark code here (copy and change it) and get going!\n","dateUpdated":"2018-07-03T12:08:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739555_-770872603","id":"20180306-081459_1983126059","dateCreated":"2018-07-03T12:08:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1278"},{"text":"%spark\n","dateUpdated":"2018-07-03T12:24:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530619739555_-770872603","id":"20180306-081532_991110953","dateCreated":"2018-07-03T12:08:59+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1279","user":"mapr"}],"name":"Bootcamp - 7 - Spark Introduction","id":"2DKV5CVD5","angularObjects":{"2DKENN16V:shared_process":[],"2DJ4YM6XQ:shared_process":[],"2DKRTNUT6:shared_process":[],"2DJ7PBUMW:shared_process":[],"2DJF51Q5T:shared_process":[],"2DJD1GWD3:shared_process":[],"2DKMUAV16:shared_process":[],"2DHRRMVA7:shared_process":[],"2DJJKUARJ:shared_process":[],"2DKTWQ79U:shared_process":[],"2DJUJ2TS2:shared_process":[],"2DH5FHR63:shared_process":[],"2DKX223HM:shared_process":[],"2DK2XZZ3Y:shared_process":[],"2DH16VXP5:shared_process":[],"2DHNPKM7Q:shared_process":[],"2DHX6YPAT:shared_process":[],"2DH3N9XPB:shared_process":[],"2DM8HBQ98:shared_process":[],"2DKU6CW45:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}